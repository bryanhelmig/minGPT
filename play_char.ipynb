{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some shakespear, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hello!\n"
    }
   ],
   "source": [
    "print('hello!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
    "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
    "        chunk = self.data[i:i+self.block_size+1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "data has 1115394 characters, 65 unique.\n"
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/17/2020 15:16:57 - INFO - mingpt.model -   number of parameters: 2.535219e+07\n"
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "check...\n"
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=200, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! which is the business so harm!\n",
      "Well, lords, and save yourselves; and no oath to be angry\n",
      "That in their embraces: and, to brave the life\n",
      "We have forgot and bandy as that time\n",
      "Have told me and he bids me for this excellent,\n",
      "Now I would say he looks on the banks\n",
      "And give more strength than a wild and provide\n",
      "A salt that with some friendly vow,\n",
      "That from the reaches of the gain and stop the sleeves\n",
      "Do scope that which He should hide for his guard\n",
      "As miser made thee first way from his holy exercise.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Go, rating to London, with all these woful chances\n",
      "Misthink the king and not be satisfied!\n",
      "\n",
      "Son:\n",
      "Was ever son so rued a father's death?\n",
      "\n",
      "Father:\n",
      "The warn's idle buy and blows: and then to make a\n",
      "fire, sir, I will keep my capss with stars out\n",
      "And safely point of good content.\n",
      "Signior Lucentio, let us hence; good gods rest ourselves:\n",
      "We shall we show her own heaven and the king\n",
      "In me resolved: I have seen a lady's nose\n",
      "That has been blue, but not her eyebrows.\n",
      "\n",
      "First Lady:\n",
      "Hark ye;\n",
      "The queen your mother rounds apace: we shall\n",
      "Present our services to a fine new prince\n",
      "One of these days; and then you'ld wanton with us,\n",
      "If we would have you.\n",
      "\n",
      "Second Lady:\n",
      "She is spread of late\n",
      "Into a goodly bulk: good time encounter her!\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "I swear.\n",
      "\n",
      "THOMAS MOWBRAY:\n",
      "And I, to keep all this.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Sweet peace conduct his sweet soul to the bosom\n",
      "Of good old Abraham! Lords appellants,\n",
      "Your differences shall all rest under gage\n",
      "Till we assign you to your days of trial.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Sweet York, what wilt thou do?\n",
      "Wilt thou not hide the trespass of thine own?\n",
      "Have we more sons? or are we like to have?\n",
      "Is not my teeming date drunk up with time?\n",
      "And wilt thou pluck my fair son from mine age,\n",
      "And rob me of a happy mother's name?\n",
      "Is he not like thee? is he not thine own?\n",
      "\n",
      "DORCAS:\n",
      "Whither?\n",
      "\n",
      "MOPSA:\n",
      "It becomes thy oath full well,\n",
      "Thou to me thy secrets tell.\n",
      "\n",
      "DORCAS:\n",
      "Me too, let me go thither.\n",
      "\n",
      "MOPSA:\n",
      "Or thou goest to the orange or mill.\n",
      "\n",
      "DORCAS:\n",
      "If to either, tho\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level shakespear\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=0.9, sample=True, top_k=5)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python_defaultSpec_1597702561008"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}